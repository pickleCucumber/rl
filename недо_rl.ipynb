{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac915eaa",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15715e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff8dbe",
   "metadata": {},
   "source": [
    "## своего рода окружение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402bc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#список наших бандитов. Бандит №4 наиболее оптимален для выбора\n",
    "bandits = [0.2,0,-0.2,-5]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit):\n",
    "    #генерим случайное число\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        #победа\n",
    "        return 1\n",
    "    else:\n",
    "        #лохпидор\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef549d",
   "metadata": {},
   "source": [
    "## своего рода агент (состоит из набора значений для бандитов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f1d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.disable_eager_execution()# вот тут сделала плохую вещь, так не надо, а то может быть не жоск типировано\n",
    "\n",
    "\n",
    "\n",
    "#оформляем feed-forward часть нейросети и происходит выбор действия\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights,0)\n",
    "\n",
    "#тута нейросеть принимает на вход действие и его результат, чтобы оценить функцию потерь и обновить веса сети\n",
    "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights,action_holder,[1])\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a6078a",
   "metadata": {},
   "source": [
    "## околообучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98658a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 02:10:46.000962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-05-15 02:10:46.001042: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-05-15 02:10:46.001080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n",
      "2024-05-15 02:10:46.002326: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общий выигрыш бандитов сейчас равен 4 bandits: [-1.  0.  0.  0.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [-1. -2. -2. 28.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [-1. -3. -1. 74.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -1.  -4.  -2. 120.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -2.  -4.  -3. 168.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -1.  -4.  -2. 214.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [  1.  -4.  -4. 258.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [  1.  -4.  -3. 303.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [  2.  -3.  -4. 350.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [  1.  -2.  -4. 398.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [  0.  -2.  -3. 446.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -2.  -4.  -1. 488.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -1.  -6.   1. 531.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -2.  -7.   1. 575.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -1.  -8.   1. 621.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -1.  -8.   1. 669.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -1. -10.   0. 714.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -2.  -9.   0. 762.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -2. -11.   0. 810.]\n",
      "Общий выигрыш бандитов сейчас равен 4 bandits: [ -2. -11.   0. 856.]\n",
      "Агент думает(Саймон говорит), лучший номер 4\n",
      "по факту\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #кол-во итераций обучения\n",
    "total_reward = np.zeros(num_bandits) #обозначаем начальный выигрыш всех бандитов равен 0\n",
    "e = 0.1 #вер-ть случайного выбора\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Запускаем граф tensorflow\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        #выбираем действие либо случайно либо на основе нейросети\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        #получаем результат, выбрав одного \n",
    "        reward = pullBandit(bandits[action]) \n",
    "        \n",
    "        #обновляем веса\n",
    "        _,resp,ww = sess.run([update,responsible_weight,weights], \n",
    "                      feed_dict={reward_holder:[reward],action_holder:[action]})\n",
    "        \n",
    "        #обновляем общий выигрыш каждого бандита\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print(\"Общий выигрыш бандитов сейчас равен \" + str(num_bandits) + \n",
    "            \" bandits: \" + str(total_reward))\n",
    "        i+=1\n",
    "print(\"Агент думает(Саймон говорит), лучший номер \" + str(np.argmax(ww)+1))\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print(\"по факту\")\n",
    "else:\n",
    "    print(\"чет не то сделали\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0e74b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
